{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kopia notatnika flight_delays.ipynb","provenance":[{"file_id":"https://github.com/verakai/DS/blob/master/flight_delays.ipynb","timestamp":1599763096534}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2fqdZNqv46LS","colab_type":"text"},"source":["# **Big Data with Spark in Google Colab**"]},{"cell_type":"markdown","metadata":{"id":"0Syk1cKh_nsb","colab_type":"text"},"source":["## Spark and Colaboratory setup"]},{"cell_type":"code","metadata":{"id":"eTsyMSUy17fM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":241},"outputId":"d5796ded-2097-4e92-d17e-a11629948871"},"source":["# Install spark-related depdencies for Python\n","\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!wget -q http://apache.osuosl.org/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz\n","!tar xf spark-2.4.3-bin-hadoop2.7.tgz\n","\n","!pip install -q findspark\n","!pip install pyspark"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/98/244399c0daa7894cdf387e7007d5e8b3710a79b67f3fd991c0b0b644822d/pyspark-2.4.3.tar.gz (215.6MB)\n","\u001b[K     |████████████████████████████████| 215.6MB 98kB/s \n","\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n","\u001b[K     |████████████████████████████████| 204kB 40.7MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Stored in directory: /root/.cache/pip/wheels/8d/20/f0/b30e2024226dc112e256930dd2cd4f06d00ab053c86278dcf3\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.7 pyspark-2.4.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mzb8TQXf2DWr","colab_type":"code","colab":{}},"source":["# Set up required environment variables\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.3-bin-hadoop2.7\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PzUr-r3Y2F6r","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"20fbc0b6-9a7c-4107-82b2-1884811da64d"},"source":["# Point Colaboratory to your Google Drive\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"E9Wjzwcc_swK","colab_type":"text"},"source":["## Data download to Google Drive"]},{"cell_type":"code","metadata":{"id":"wovKYYQM4vMt","colab_type":"code","colab":{}},"source":["# Download datasets directly to your Google Drive \"Colab Datasets\" folder\n","\n","import requests\n","\n","# 2007 data\n","\n","file_url = \"http://stat-computing.org/dataexpo/2009/2007.csv.bz2\"\n","\n","r = requests.get(file_url, stream = True) \n","\n","with open(\"/content/gdrive/My Drive/Colab Datasets/2007.csv.bz2\", \"wb\") as file: \n","\tfor block in r.iter_content(chunk_size = 1024): \n","\t\tif block: \n","\t\t\tfile.write(block)\n","\n","# 2008 data\n","\n","file_url = \"http://stat-computing.org/dataexpo/2009/2008.csv.bz2\"\n","\n","r = requests.get(file_url, stream = True) \n","\n","with open(\"/content/gdrive/My Drive/Colab Datasets/2008.csv.bz2\", \"wb\") as file: \n","\tfor block in r.iter_content(chunk_size = 1024): \n","\t\tif block: \n","\t\t\tfile.write(block)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XMnnHpLcDZKw","colab_type":"text"},"source":["##  **Import** tools from PySpark\n"]},{"cell_type":"code","metadata":{"id":"lgTbo49l2H_D","colab_type":"code","colab":{}},"source":["# Tools we need to connect to the Spark server, load our data, clean it, and prepare, execute, and evaluate a model\n","\n","from pyspark import SparkContext\n","from pyspark.sql import SparkSession\n","\n","from pyspark.ml import Pipeline\n","from pyspark.ml.classification import RandomForestClassifier\n","from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","from pyspark.sql.functions import isnan, when, count, col"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gnY4vcKuDDvo","colab_type":"text"},"source":["## Set Constants"]},{"cell_type":"code","metadata":{"id":"ypM4p5ti9i-s","colab_type":"code","colab":{}},"source":["CSV_2007= \"/content/gdrive/My Drive/Colab Datasets/2007.csv.bz2\" \n","CSV_2008= \"/content/gdrive/My Drive/Colab Datasets/2008.csv.bz2\"\n","APP_NAME = \"Flight Delays\"\n","SPARK_URL = \"local[*]\"\n","RANDOM_SEED = 141109\n","TRAINING_DATA_RATIO = 0.7\n","RF_NUM_TREES = 8\n","RF_MAX_DEPTH = 4\n","RF_NUM_BINS = 32"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N0FRbSOUDMRJ","colab_type":"text"},"source":["## Connect to the server and load data"]},{"cell_type":"code","metadata":{"id":"I5AlE_w-3jUg","colab_type":"code","colab":{}},"source":["# Connect to the Spark server\n","\n","spark = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()\n","\n","# Load datasets\n","\n","df_2007 = spark.read.options(header=\"true\",inferschema = \"true\").csv(CSV_2007)\n","df_2008 = spark.read.options(header=\"true\",inferschema = \"true\").csv(CSV_2008)\n","\n","# We concatenate both datasets\n","\n","df = df_2007.unionAll(df_2008)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bMND_BX1ewlB","colab_type":"text"},"source":["## Prepare, clean and validate the data\n","\n"]},{"cell_type":"code","metadata":{"id":"RcuffbHxe1ae","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"45c64597-81ec-4970-e2a4-87976aaa52b9"},"source":["# What's the data shape before starting cleaning ?\n","\n","print(f\"The shape is {df.count():d} rows by {len(df.columns):d} columns.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The shape is 14462943 rows by 29 columns.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H6wY9rQjgU5o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9db2ae60-a864-40e6-a506-7e99431b5e43"},"source":["# What's the number of null values ?\n","\n","null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n","                         for c in df.columns]).toPandas().to_dict(orient='records')\n","\n","print(f\"We have {sum(null_counts[0].values()):d} null values in this dataset.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["We have 14248147 null values in this dataset.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zAOe1qE-gfOs","colab_type":"code","colab":{}},"source":["# Drop null columns and inputs ?\n","\n","df = df.drop(df.CancellationCode)\n","df = df.na.drop()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2rnFflugfSL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3667d0b0-fb34-4746-a25a-da93c8fa67ee"},"source":["# Confirm there are no null values\n","\n","null_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) \n","                         for c in df.columns]).toPandas().to_dict(orient='records')\n","\n","print(f\"We have {sum(null_counts[0].values()):d} null values in this dataset.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["We have 0 null values in this dataset.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a3oIOyVSWQb2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"08778c71-6df8-4503-a2e7-41e5be5d6a25"},"source":["# What's the data shape after cleaning ?\n","\n","print(f\"The shape is {df.count():d} rows by {len(df.columns):d} columns.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The shape is 14379556 rows by 28 columns.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iQyE4UJJI1i8","colab_type":"text"},"source":["## Set up and run our classifier in Spark"]},{"cell_type":"code","metadata":{"id":"e98yIqdmKWcK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":493},"outputId":"5161e94d-f05f-45c0-f25c-2b30c32afe6c"},"source":["# What are the column's type ?\n","\n","df.dtypes"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Year', 'int'),\n"," ('Month', 'int'),\n"," ('DayofMonth', 'int'),\n"," ('DayOfWeek', 'int'),\n"," ('DepTime', 'string'),\n"," ('CRSDepTime', 'int'),\n"," ('ArrTime', 'string'),\n"," ('CRSArrTime', 'int'),\n"," ('UniqueCarrier', 'string'),\n"," ('FlightNum', 'int'),\n"," ('TailNum', 'string'),\n"," ('ActualElapsedTime', 'string'),\n"," ('CRSElapsedTime', 'string'),\n"," ('AirTime', 'string'),\n"," ('ArrDelay', 'string'),\n"," ('DepDelay', 'string'),\n"," ('Origin', 'string'),\n"," ('Dest', 'string'),\n"," ('Distance', 'int'),\n"," ('TaxiIn', 'string'),\n"," ('TaxiOut', 'string'),\n"," ('Cancelled', 'int'),\n"," ('Diverted', 'int'),\n"," ('CarrierDelay', 'string'),\n"," ('WeatherDelay', 'string'),\n"," ('NASDelay', 'string'),\n"," ('SecurityDelay', 'string'),\n"," ('LateAircraftDelay', 'string')]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"UPJdcVZhI4Sx","colab_type":"code","colab":{}},"source":["# Create list of feature columns\n","\n","feature_cols = ['Year', 'Month', 'DayofMonth', 'DayOfWeek', 'CRSDepTime', \n","                'CRSArrTime', 'FlightNum', 'Distance', 'Diverted']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4ZETOBtKKlP","colab_type":"code","colab":{}},"source":["# Generate and create our new feature vector column\n","\n","df = VectorAssembler(inputCols=feature_cols, outputCol=\"features\").transform(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bgWPhJ9XK3Fo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"3313ee9d-d191-4eb7-f415-796a90f3303b"},"source":["# Select input columns\n","\n","df.select(\"Cancelled\", \"features\").show(5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["+---------+--------------------+\n","|Cancelled|            features|\n","+---------+--------------------+\n","|        0|[2007.0,1.0,1.0,1...|\n","|        0|[2007.0,1.0,1.0,1...|\n","|        0|[2007.0,1.0,1.0,1...|\n","|        0|[2007.0,1.0,1.0,1...|\n","|        0|[2007.0,1.0,1.0,1...|\n","+---------+--------------------+\n","only showing top 5 rows\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FZrvTdxBLA2-","colab_type":"code","colab":{}},"source":["# Build the training indexers\n","\n","# Generate a labelIndexer\n","labelIndexer = StringIndexer(inputCol=\"Cancelled\", outputCol=\"indexedLabel\").fit(df)\n","\n","# Generate the indexed feature vector\n","featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df)\n","    \n","# Split the data into training and tests sets\n","(trainingData, testData) = df.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])\n","\n","# Train the RandomForest model\n","rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=RF_NUM_TREES)\n","\n","# Chain indexers and the forest models in a Pipeline\n","pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hjE_avVbLfmP","colab_type":"code","colab":{}},"source":["# Train the model\n","\n","model = pipeline.fit(trainingData)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SderkFFTSNw2","colab_type":"code","colab":{}},"source":["# Make predictions\n","\n","predictions = model.transform(testData)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sSOdyvSpLl9e","colab_type":"text"},"source":["## Evaluate our model"]},{"cell_type":"code","metadata":{"id":"NTGwK2jkLlKJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"fcb7ca99-7db0-44e5-fe70-1bcbed707638"},"source":["# Select prediction, true label and compute test error\n","evaluator = MulticlassClassificationEvaluator(\n","    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = evaluator.evaluate(predictions)\n","\n","print(f\"Test Error = {(1.0 - accuracy):g}\")\n","print(f\"Accuracy = {accuracy:g}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Error = 0.0149426\n","Accuracy = 0.985057\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mtaUvH7XSFpQ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}